# VELORA - Gu√≠a de Componentes y Desarrollo
## Referencia T√©cnica para AI Engineers

---

# 1. INVENTARIO COMPLETO DE COMPONENTES

## 1.1 √Årbol de Directorios Anotado

```
velora_auto/
‚îÇ
‚îú‚îÄ‚îÄ üìÅ app/                              # CAPA DE PRESENTACI√ìN
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ streamlit_app.py                 # UI principal (2355 l√≠neas)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ render_velora_header()       # Header corporativo
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ render_sidebar_logo()        # Logo sidebar
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ render_agentic_interview()   # Chat Fase 2 con streaming
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ display_phase1_results()     # Resultados Fase 1
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ display_final_results()      # Resultados finales
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main()                       # Orquestador de tabs
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ assets/                       # Recursos est√°ticos
‚îÇ       ‚îú‚îÄ‚îÄ Velora_logotipo_*.png        # Logos corporativos
‚îÇ       ‚îî‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ üìÅ src/evaluator/                    # N√öCLEO DEL SISTEMA
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                      # Exports p√∫blicos del paquete
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __version__ = "2.0.0"
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models.py                        # MODELOS DE DATOS (149 l√≠neas)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RequirementType (Enum)       # obligatory | optional
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ConfidenceLevel (Enum)       # high | medium | low
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Requirement                  # Modelo de requisito
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ExtractedRequirement         # Structured Output: extracci√≥n
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RequirementMatch             # Structured Output: matching
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ResponseEvaluation           # Structured Output: evaluaci√≥n
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Phase1Result                 # Resultado Fase 1
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ EvaluationResult             # Resultado final completo
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ core/                         # L√ìGICA DE NEGOCIO
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                  # Exports: Evaluator, Analyzer, Interviewer
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py                 # ORQUESTADOR (402 l√≠neas)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ CandidateEvaluator
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__()           # Inicializa Analyzer + Interviewer
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ evaluate_candidate() # Flujo completo
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ reevaluate_with_interview()  # Re-c√°lculo post Fase 2
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ record_feedback()    # LangSmith feedback
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyzer.py                  # FASE 1 (516 l√≠neas)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Phase1Analyzer
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__()           # Config LLM, semantic matcher
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ extract_requirements()       # Extracci√≥n de oferta
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ match_cv_with_requirements() # Matching CV
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ analyze()            # Flujo completo Fase 1
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ _analyze_traditional() / _analyze_with_langgraph()
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agentic_interviewer.py       # FASE 2 AG√âNTICA (503 l√≠neas)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ AgenticInterviewer
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ initialize_interview()       # Setup estado
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ stream_greeting()    # Saludo con streaming
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ stream_question()    # Pregunta con streaming
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ stream_closing()     # Cierre con streaming
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ register_response()  # Registrar respuesta
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ evaluate_response()  # Evaluar cumplimiento
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ validate_coverage()  # Auditor√≠a de cobertura
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py                # SEMANTIC MATCHER (234 l√≠neas)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SemanticMatcher
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ index_cv()           # Indexar CV en FAISS
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ find_evidence()      # Buscar evidencia sem√°ntica
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ clear()              # Limpiar vectorstore
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ graph.py                     # LANGGRAPH (505 l√≠neas)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Phase1State (TypedDict)  # Estado compartido
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ create_extract_node()    # Agente extractor
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ create_embed_node()      # Agente embedder
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ create_match_node()      # Agente matcher
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ create_score_node()      # Agente scorer
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ create_phase1_graph()    # Construcci√≥n del grafo
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ run_phase1_graph_streaming()  # Ejecuci√≥n con streaming
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logging_config.py            # LOGGING OPERACIONAL (375 l√≠neas)
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ OperationalLogger (Singleton)
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ config_*()           # Logs de configuraci√≥n
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ phase1_*()           # Logs de Fase 1
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ phase2_*()           # Logs de Fase 2
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ rag_*()              # Logs de RAG
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ llm/                          # CAPA LLM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                  # Exports: LLMFactory, EmbeddingFactory
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ factory.py                   # LLM FACTORY (249 l√≠neas)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ configure_langsmith()    # Setup LangSmith
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ LLMFactory
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ create_llm()         # Crear instancia LLM
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ get_available_providers()
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ get_available_models()
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings_factory.py        # EMBEDDINGS FACTORY (233 l√≠neas)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ EmbeddingFactory
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ create_embeddings()  # Crear instancia embeddings
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ supports_embeddings()
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ get_fallback_provider()
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hyperparameters.py           # CONFIGURACI√ìN TEMP (223 l√≠neas)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LLMHyperparameters (dataclass)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PHASE1_EXTRACTION        # temp=0.0
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PHASE1_MATCHING          # temp=0.1
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PHASE2_INTERVIEW         # temp=0.3
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RAG_CHATBOT              # temp=0.4
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ HyperparametersConfig    # Acceso centralizado
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompts.py                   # PROMPTS CENTRALIZADOS (164 l√≠neas)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ EXTRACT_REQUIREMENTS_PROMPT
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ MATCH_CV_REQUIREMENTS_PROMPT
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ GENERATE_QUESTIONS_PROMPT
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ EVALUATE_RESPONSE_PROMPT
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ AGENTIC_SYSTEM_PROMPT
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ AGENTIC_GREETING_PROMPT
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ AGENTIC_QUESTION_PROMPT
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ AGENTIC_CLOSING_PROMPT
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ rag/                          # RETRIEVAL AUGMENTED GENERATION
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chatbot.py                   # RAG CHATBOT (309 l√≠neas)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ HistoryChatbot
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ query()              # Consulta simple
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ query_with_history() # Con contexto conversaci√≥n
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ get_quick_summary()  # Stats sin LLM
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vectorstore.py               # VECTORSTORE (330 l√≠neas)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ normalize_text_for_embedding()
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ HistoryVectorStore
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ index_evaluations()  # Indexar historial
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ search()             # B√∫squeda sem√°ntica
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ rebuild_index()      # Re-indexar
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ storage/                      # PERSISTENCIA
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ memory.py                    # USER MEMORY (513 l√≠neas)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ EnrichedEvaluation (Pydantic)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ extract_job_title()      # Heur√≠stica de t√≠tulo
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ create_enriched_evaluation()
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ UserMemory
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ save_evaluation()
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ get_evaluations()
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ get_searchable_texts()
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ extraction/                   # UTILIDADES DE ENTRADA
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf.py                       # PDF EXTRACTOR (50 l√≠neas)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ extract_text_from_pdf()
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ url.py                       # WEB SCRAPER (504 l√≠neas)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ _scrape_with_requests()  # HTTP b√°sico
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ _scrape_with_browser()   # Playwright
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ scrape_job_offer_url()   # API p√∫blica
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ processing/                   # UTILIDADES DE PROCESO
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ validation.py                # VALIDACIONES (55 l√≠neas)
‚îÇ           ‚îú‚îÄ‚îÄ calculate_score()        # C√°lculo de puntuaci√≥n
‚îÇ           ‚îî‚îÄ‚îÄ load_text_file()
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/                             # DATOS PERSISTENTES
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ user_memory/                  # JSON por usuario
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ {user_id}.json
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ vectors/                      # √çndices FAISS por usuario
‚îÇ       ‚îî‚îÄ‚îÄ {user_id}/
‚îÇ           ‚îú‚îÄ‚îÄ index.faiss
‚îÇ           ‚îú‚îÄ‚îÄ index.pkl
‚îÇ           ‚îî‚îÄ‚îÄ embedding_provider.txt   # Compatibilidad
‚îÇ
‚îú‚îÄ‚îÄ üìÅ docs/                             # DOCUMENTACI√ìN
‚îÇ   ‚îú‚îÄ‚îÄ VELORA_SYSTEM_ARCHITECTURE.md    # Este documento
‚îÇ   ‚îú‚îÄ‚îÄ AGENTIC_INTERVIEWER.md
‚îÇ   ‚îú‚îÄ‚îÄ EMBEDDINGS_ARCHITECTURE.md
‚îÇ   ‚îú‚îÄ‚îÄ HYPERPARAMETERS_GUIDE.md
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                     # Dependencias pip
‚îú‚îÄ‚îÄ pyproject.toml                       # Configuraci√≥n proyecto
‚îú‚îÄ‚îÄ run_app.py                           # Script de ejecuci√≥n
‚îî‚îÄ‚îÄ README.md
```

---

# 2. GU√çA DE PATRONES IMPLEMENTADOS

## 2.1 Singleton Pattern - OperationalLogger

```python
# logging_config.py
class OperationalLogger:
    _instance: Optional['OperationalLogger'] = None
    _initialized: bool = False
    
    def __new__(cls) -> 'OperationalLogger':
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if OperationalLogger._initialized:
            return  # Evita re-inicializaci√≥n
        OperationalLogger._initialized = True
        # ... setup ...

# Uso global
op_logger = OperationalLogger()

def get_operational_logger() -> OperationalLogger:
    return op_logger
```

**Justificaci√≥n**: Un √∫nico logger para todo el sistema garantiza:
- Consistencia en formato
- Sin duplicaci√≥n de handlers
- Estado compartido

---

## 2.2 Factory Pattern - LLMFactory / EmbeddingFactory

```python
# factory.py
class LLMFactory:
    @staticmethod
    def create_llm(
        provider: str,      # "openai" | "google" | "anthropic"
        model_name: str,
        temperature: float = 0.1,
        api_key: Optional[str] = None
    ) -> BaseChatModel:
        
        if provider == "openai":
            return ChatOpenAI(model=model_name, temperature=temperature, ...)
        elif provider == "google":
            return ChatGoogleGenerativeAI(model=model_name, ...)
        elif provider == "anthropic":
            return ChatAnthropic(model=model_name, ...)
        else:
            raise ValueError(f"Proveedor no v√°lido: {provider}")
```

**Justificaci√≥n**: Desacopla creaci√≥n de uso:
- C√≥digo de negocio no conoce el proveedor
- Cambio de proveedor = cambio de config
- Detecci√≥n din√°mica de disponibilidad

---

## 2.3 Pydantic Structured Output Pattern

```python
# models.py
class CVMatchingResponse(BaseModel):
    """Respuesta del LLM para matching CV-requisitos"""
    matches: List[RequirementMatch] = Field(
        ..., description="Lista de resultados de matching"
    )
    analysis_summary: str = Field(..., description="Resumen del an√°lisis")

# analyzer.py
class Phase1Analyzer:
    def __init__(self, ...):
        # Crear LLM con structured output
        self.matching_llm = self.llm.with_structured_output(CVMatchingResponse)
    
    def match_cv_with_requirements(self, cv, requirements):
        chain = prompt | self.matching_llm
        result: CVMatchingResponse = chain.invoke({...})
        # result es SIEMPRE un CVMatchingResponse v√°lido
        return result.matches
```

**Justificaci√≥n**: Elimina parsing manual:
- Garant√≠a de estructura
- Validaci√≥n autom√°tica
- Type hints para IDE

---

## 2.4 Generator/Streaming Pattern

```python
# agentic_interviewer.py
def stream_question(self, question_idx: int) -> Generator[str, None, None]:
    """Genera pregunta con streaming token-by-token"""
    
    chain = prompt | self.llm | StrOutputParser()
    
    question_text = ""
    for chunk in chain.stream({}):  # STREAMING REAL
        question_text += chunk
        yield chunk  # Token a token
    
    # Post-procesamiento despu√©s del streaming
    self._conversation_history.append({...})

# streamlit_app.py
for token in interviewer.stream_question(idx):
    full_question += token
    container.markdown(f"**{full_question}**|")  # Cursor parpadeante
```

**Justificaci√≥n**: UX moderna tipo ChatGPT:
- Feedback visual inmediato
- Sensaci√≥n de "agente pensando"
- Latencia percibida reducida

---

## 2.5 State Machine Pattern - AgenticInterviewer

```python
# Estados del agente
class AgenticInterviewer:
    def initialize_interview(...):
        # Estado: INITIALIZED
        self._pending_requirements = [...]
        self._current_idx = 0
        self._conversation_history = []
    
    def stream_greeting(...):
        # Estado: GREETING ‚Üí QUESTIONING
        ...
        
    def stream_question(question_idx):
        # Estado: QUESTIONING (iterativo)
        self._pending_requirements[question_idx]["asked"] = True
        ...
    
    def register_response(question_idx, response):
        # Transici√≥n condicional
        self._pending_requirements[question_idx]["answered"] = True
        is_complete = question_idx + 1 >= len(self._pending_requirements)
        # Si complete ‚Üí Estado: CLOSING
    
    def stream_closing(...):
        # Estado: CLOSING ‚Üí COMPLETE
        ...
```

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              M√ÅQUINA DE ESTADOS - ENTREVISTA                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇINITIALIZED‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ GREETING  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇQUESTIONING‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  CLOSING  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                          ‚îÇ                 ‚îÇ       ‚îÇ
‚îÇ                                          ‚îÇ (loop)          ‚îÇ       ‚îÇ
‚îÇ                                          ‚ñº                 ‚ñº       ‚îÇ
‚îÇ                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ                                    ‚îÇ  WAITING  ‚îÇ    ‚îÇ COMPLETE  ‚îÇ ‚îÇ
‚îÇ                                    ‚îÇ RESPONSE  ‚îÇ    ‚îÇ           ‚îÇ ‚îÇ
‚îÇ                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

# 3. C√ìMO EXTENDER EL SISTEMA

## 3.1 Agregar Nuevo Proveedor LLM

```python
# 1. Verificar disponibilidad (factory.py)
try:
    from langchain_nuevo_proveedor import ChatNuevoProveedor
    NUEVO_AVAILABLE = True
except ImportError:
    NUEVO_AVAILABLE = False
    ChatNuevoProveedor = None

# 2. Agregar a la factory
class LLMFactory:
    NUEVO_MODELS = ["modelo-1", "modelo-2"]
    
    @staticmethod
    def create_llm(...):
        ...
        elif provider == "nuevo":
            if not NUEVO_AVAILABLE:
                raise ImportError("...")
            return ChatNuevoProveedor(model=model_name, ...)

# 3. Actualizar UI (streamlit_app.py)
# La UI detecta autom√°ticamente via get_available_providers()
```

## 3.2 Agregar Nueva Fase de Evaluaci√≥n

```python
# 1. Crear nuevo m√≥dulo en core/
# core/phase3_technical.py
class Phase3TechnicalTest:
    def __init__(self, llm, ...):
        ...
    
    def generate_test(self, requirements: List[Requirement]):
        """Genera test t√©cnico basado en requisitos"""
        ...
    
    def evaluate_test(self, candidate_answers: List[str]):
        """Eval√∫a respuestas del test t√©cnico"""
        ...

# 2. Integrar en evaluator.py
class CandidateEvaluator:
    def __init__(self, ...):
        ...
        self.phase3_tester = Phase3TechnicalTest(...)
    
    def evaluate_with_technical_test(self, phase2_result, test_responses):
        """Ejecuta Fase 3 si se requiere"""
        ...

# 3. Agregar modelos necesarios en models.py
class TechnicalQuestion(BaseModel):
    ...

class Phase3Result(BaseModel):
    ...

# 4. Agregar prompts en prompts.py
GENERATE_TECHNICAL_TEST_PROMPT = """..."""
```

## 3.3 Agregar Nuevo Tipo de Entrada

```python
# 1. Crear extractor en extraction/
# extraction/linkedin.py
def extract_from_linkedin_url(url: str) -> Optional[str]:
    """Extrae perfil de LinkedIn"""
    ...

# 2. Exportar en extraction/__init__.py
from .linkedin import extract_from_linkedin_url

# 3. Integrar en UI
if input_type == "linkedin":
    cv_text = extract_from_linkedin_url(url)
```

---

# 4. GU√çA DE TROUBLESHOOTING

## 4.1 Errores Comunes y Soluciones

### Error: "Structured Output failed"
```
LangChain error: Output parsing failed
```

**Causa**: El LLM no gener√≥ JSON v√°lido para el modelo Pydantic.

**Soluci√≥n**:
1. Verificar que el modelo soporta structured output
2. Bajar la temperatura para mayor determinismo
3. Revisar el prompt por ambig√ºedades

```python
# Verificar modelo
print(f"Modelo: {model_name}")
print(f"Temp: {temperature}")

# Test con temperatura 0
llm = LLMFactory.create_llm(provider, model, temperature=0.0)
```

### Error: "Embedding dimension mismatch"
```
FAISS error: Index dimension mismatch
```

**Causa**: El √≠ndice fue creado con un proveedor de embeddings diferente.

**Soluci√≥n**:
```python
# Forzar re-indexaci√≥n
vectorstore = HistoryVectorStore(user_id, embedding_provider="openai")
vectorstore.rebuild_index(evaluations)

# O eliminar √≠ndice manualmente
# data/vectors/{user_id}/ ‚Üí DELETE
```

### Error: "API key not found"
```
AuthenticationError: No API key provided
```

**Soluci√≥n**:
```python
# 1. Via variable de entorno
export OPENAI_API_KEY="sk-..."

# 2. Via archivo .env en ra√≠z
OPENAI_API_KEY=sk-...

# 3. Via par√°metro expl√≠cito
llm = LLMFactory.create_llm(..., api_key="sk-...")
```

### Error: "Playwright not installed"
```
Error: Playwright chromium not installed
```

**Soluci√≥n**:
```bash
pip install playwright
playwright install chromium
```

---

# 5. M√âTRICAS Y OBSERVABILIDAD

## 5.1 LangSmith Integration

```python
# Habilitado por defecto si hay LANGSMITH_API_KEY
from llm.factory import configure_langsmith, get_langsmith_client

# En evaluator.py
langsmith = configure_langsmith(project_name="velora-evaluator")

# Registro de feedback
evaluator.record_feedback(
    score=0.9,  # 0-1
    comment="Evaluaci√≥n precisa",
    run_id=last_run_id
)
```

## 5.2 Logs Operacionales

```python
# Habilitar/deshabilitar
from core.logging_config import get_operational_logger
op_logger = get_operational_logger()
op_logger.enabled = False  # Silenciar logs

# Logs disponibles
op_logger.phase1_start(mode="langgraph")
op_logger.extraction_complete(total=12, obligatory=5, optional=7)
op_logger.matching_complete(fulfilled=8, unfulfilled=4, score=66.7)
op_logger.phase1_complete(discarded=False, score=66.7, duration_ms=4200)
```

## 5.3 M√©tricas de Cobertura

```python
# Validar cobertura de entrevista
coverage = interviewer.validate_coverage()
print(f"Cobertura: {coverage['coverage_percentage']:.1f}%")
print(f"Sin preguntar: {coverage['uncovered_requirements']}")
assert coverage['is_complete'], "Cobertura incompleta!"
```

---

# 6. CHECKLIST DE CONTRIBUCI√ìN

## 6.1 Antes de Cada PR

- [ ] Ejecutar `python -m pytest` (si hay tests)
- [ ] Verificar imports en `__init__.py`
- [ ] Actualizar `__all__` si se agregan exports
- [ ] Documentar nuevas funciones con docstrings
- [ ] Verificar que no hay API keys hardcodeadas
- [ ] Probar con m√∫ltiples proveedores (OpenAI, Google)
- [ ] Verificar que UI no se rompe con errores

## 6.2 Convenciones de Commit

```
feat: Nueva funcionalidad
fix: Correcci√≥n de bug
docs: Documentaci√≥n
refactor: Refactorizaci√≥n sin cambio funcional
test: Tests
chore: Mantenimiento

Ejemplo:
feat(phase2): Agregar streaming en preguntas de entrevista
fix(analyzer): Corregir parsing de requisitos duplicados
docs: Actualizar gu√≠a de contribuci√≥n
```

---

# 7. REFERENCIAS R√ÅPIDAS

## 7.1 Comandos de Desarrollo

```bash
# Ejecutar aplicaci√≥n
python run_app.py

# O directamente
streamlit run app/streamlit_app.py

# Instalar dependencias
pip install -r requirements.txt

# Instalar Playwright (para scraping)
playwright install chromium
```

## 7.2 Variables de Entorno

```bash
# Requeridas (al menos una)
OPENAI_API_KEY=sk-...
GOOGLE_API_KEY=...
ANTHROPIC_API_KEY=sk-ant-...

# Opcionales
LANGSMITH_API_KEY=...              # Para trazabilidad
LANGCHAIN_TRACING_V2=true          # Auto-habilitado por LangSmith
LANGCHAIN_PROJECT=velora-evaluator # Nombre proyecto LangSmith
```

## 7.3 Rutas de Datos

```
data/user_memory/{user_id}.json    # Historial de evaluaciones
data/vectors/{user_id}/            # √çndices FAISS
  ‚îú‚îÄ‚îÄ index.faiss
  ‚îú‚îÄ‚îÄ index.pkl
  ‚îî‚îÄ‚îÄ embedding_provider.txt
```

---

*Gu√≠a de Componentes - Velora v2.0*
*Para uso interno de AI Engineers*

